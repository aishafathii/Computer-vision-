# -*- coding: utf-8 -*-
"""Untitled19.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1fUKg4kEUzyjs5q3mZeDkQyS8u5wAKNO0
"""

import numpy as np
import cv2
from matplotlib import pyplot as plt
from tqdm import tqdm
from itertools import combinations

import numpy as np
import matplotlib.pyplot as plt


from google.colab.patches import cv2_imshow
# Load our images
from google.colab import files
files.upload()

def matchfeatures(img1, img2, nfeatures=1000):
    # Create our ORB detector and detect keypoints and descriptors
    orb = cv2.ORB_create(nfeatures=2000)

    # Find the key points and descriptors with ORB
    keypoints1, descriptors1 = orb.detectAndCompute(img1, None)
    keypoints2, descriptors2 = orb.detectAndCompute(img2, None)

    cv2_imshow(cv2.drawKeypoints(img1, keypoints1, None, (255, 0, 255)))
    cv2_imshow(cv2.drawKeypoints(img2, keypoints2, None, (255, 0, 255)))


    # Using BruteForce Matcher to match detected features
    bf = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=True)
    matches = bf.match(descriptors1, descriptors2)

    # Getting best features
    matches = sorted(matches, key=lambda x: x.distance)

    # Printing the matched features and diplaying them
    img3 = cv2.drawMatches(img1, keypoints1, img2, keypoints2,
                                  matches[:20], None, flags=2)
    plt.imshow(img3)
    plt.show()

    # Storing the points of matched features
    common_points = []
    for match in matches:
        x1y1 = keypoints1[match.queryIdx].pt
        x2y2 = keypoints2[match.trainIdx].pt
        feature = list(map(int, list(x1y1) + list(x2y2) + [match.distance]))
        common_points.append(feature)

    print(f"Feature Matching Results: POC(s) found: {len(common_points)}")

    return np.array(common_points)


def homography(poc):
    A = []
    for x, y, u, v, distance in poc:
        A.append([x, y, 1, 0, 0, 0, -u*x, -u*y, -u])
        A.append([0, 0, 0, x, y, 1, -v*x, -v*y, -v])
    A = np.asarray(A)

    # Taking SVD
    U, S, Vh = np.linalg.svd(A)

    L = Vh[-1, :] / Vh[-1, -1]
    H = L.reshape(3, 3)
    print(H)
    return H


def ransac(points, n=30, threshold=2, max_iterations=4000):

    assert(len(points) > n)

    best_score = 0          # To store number of inliers
    best_inliers = None     # To store inliers
    best_points = points[:n]      # Separating best points

    # To get list of 4 points out of best poc
    match_pairs = list(combinations(best_points, 4))
    # Performing Ransac
    for matches in tqdm(match_pairs[:max_iterations]):
        H = homography(matches)
        inliers = []
        count = 0
        # Caclulating number of inliers
        for feature in best_points:
            src = np.ones((3, 1))
            tgt = np.ones((3, 1))
            src[:2, 0] = feature[:2]
            tgt[:2, 0] = feature[2:4]
            # Transforming other features based on the current homography
            tgt_hat = H@src
            if tgt_hat[-1, 0] != 0:
                tgt_hat = tgt_hat/tgt_hat[-1, 0]
                if np.linalg.norm(tgt_hat-tgt) < threshold:
                    count += 1
                    inliers.append(feature)
        if count > best_score:
            best_score = count
            best_inliers = inliers

    # Caclulating Transformation based on best inliers
    best_H = homography(best_inliers)
    print(f"Homography Results: Inliers of current H: {best_score}/{n}")
    return best_H


def project(x1, y1, H):
    homogeneous_coordinates = np.array([x1, y1, 1])
    projective_transformation = H.dot(homogeneous_coordinates)
    w = projective_transformation[2]
    xp = projective_transformation[0]/w
    yp = projective_transformation[1]/w

    return xp, yp 

def stitch(img1, img2, hom, homInv):
    image2_h, image2_w, image2_ch = img2.shape

    xp1, yp1 = project(0, 0, homInv)                        # Top-Left Corner
    xp2, yp2 = project(0, image2_h-1, homInv)               # Top-Right Corner
    xp3, yp3 = project(image2_w-1, 0, homInv)               # Bottom-Left Corner
    xp4, yp4 = project(image2_w-1, image2_h-1, homInv)      # Bottom-Right Corner

    image1_h, image1_w, image1_ch = img1.shape
    min_x = np.round(min(0, xp1, xp2, xp3, xp4)).astype(int)  
    min_y = np.round(min(0, yp1, yp2, yp3, yp4)).astype(int)  
    offset_x = np.abs(min_x)   
    offset_y = np.abs(min_y)  

    max_x = np.round(max(image1_w, xp1, xp2, xp3, xp4)).astype(int)
    max_y = np.round(max(image1_h, yp1, yp2, yp3, yp4)).astype(int)

    new_width = offset_x + image1_w + np.abs(image1_w-max_x)
    new_height = offset_y + image1_h + np.abs(image1_h-max_y)

    stitched_image = np.zeros((new_height, new_width, 3))

    # Add image1 onto stitched_image
    for y in range(image1_h):
        for x in range(image1_w):
            # The offset for x is the min_x value we calculated earlier, and likewise for y
            stitched_image[y + offset_y, x + offset_x] = img1[y, x]

    # Project stitched_image onto image2
    stitched_image_h, stitched_image_w, c = stitched_image.shape
    for y in range(min_y, stitched_image_h):
        for x in range(min_x, stitched_image_w):
            if y+offset_y < stitched_image_h and x + offset_x < stitched_image_w:
                xp_stitched, yp_stitched = project(x, y, hom)
                if 0 < xp_stitched < image2_w and 0 < yp_stitched < image2_h:
                    pixel = cv2.getRectSubPix(img2, (1, 1), (xp_stitched, yp_stitched))
                    stitched_image[y+offset_y, x+offset_x] = pixel

    stitched_image = stitched_image.astype(np.uint8)

    return stitched_image


# Example of the following function
img1 = cv2.imread("1.jpg")
img2 = cv2.imread("2.jpg")
poc = matchfeatures(img1, img2)
H = ransac(poc)
HInv = np.linalg.inv(H)     # Inverse of hom
#im_out = cv2.warpPerspective(img1, img2, H, img2.size())



print("The final transformation is:")
print(H)

FFF = stitch(img1, img2, H, HInv)
plt.imshow(FFF)

